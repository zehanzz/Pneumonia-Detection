{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyNcXVFmZguhyu4ofpUfu8q4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zehanzz/Pneumonia_Detection/blob/main/Pneumonia_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4i0n6DxDG7xQ"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "!mkdir ~/.kaggle/\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip chest-xray-pneumonia.zip"
      ],
      "metadata": {
        "id": "MlNM1mEsKp4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "import cv2"
      ],
      "metadata": {
        "id": "GsXuVJlGLXM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['PNEUMONIA', 'NORMAL']\n",
        "img_size = 150\n",
        "def get_training_data(data_dir):\n",
        "    data = []\n",
        "    for label in labels:\n",
        "        path = os.path.join(data_dir, label)\n",
        "        class_num = labels.index(label)\n",
        "        for img in os.listdir(path):\n",
        "            try:\n",
        "                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
        "                resized_arr = cv2.resize(img_arr, (img_size, img_size)) # Reshaping images to preferred size\n",
        "                data.append([resized_arr, class_num])\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "    return np.array(data)"
      ],
      "metadata": {
        "id": "UzhxFotFLpem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "woUPm0iuLyHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = get_training_data('/content/chest_xray/chest_xray/train')\n",
        "test = get_training_data('/content/chest_xray/chest_xray/test')\n",
        "val = get_training_data('/content/chest_xray/chest_xray/val')"
      ],
      "metadata": {
        "id": "r7g7u624LukL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "x_val = []\n",
        "y_val = []\n",
        "\n",
        "x_test = []\n",
        "y_test = []\n",
        "\n",
        "for feature, label in train:\n",
        "    x_train.append(feature)\n",
        "    y_train.append(label)\n",
        "\n",
        "for feature, label in test:\n",
        "    x_test.append(feature)\n",
        "    y_test.append(label)\n",
        "\n",
        "for feature, label in val:\n",
        "    x_val.append(feature)\n",
        "    y_val.append(label)"
      ],
      "metadata": {
        "id": "tefGBadHL2vB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the data\n",
        "x_train = np.array(x_train) / 255\n",
        "x_val = np.array(x_val) / 255\n",
        "x_test = np.array(x_test) / 255"
      ],
      "metadata": {
        "id": "S_QecCi5MKwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# resize data for deep learning\n",
        "x_train = x_train.reshape(-1, img_size, img_size, 1)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "x_val = x_val.reshape(-1, img_size, img_size, 1)\n",
        "y_val = np.array(y_val)\n",
        "\n",
        "x_test = x_test.reshape(-1, img_size, img_size, 1)\n",
        "y_test = np.array(y_test)"
      ],
      "metadata": {
        "id": "FHHEacPZMNeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# With data augmentation to prevent overfitting and handling the imbalance in dataset\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.2, # Randomly zoom image\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip = True,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "\n",
        "datagen.fit(x_train)"
      ],
      "metadata": {
        "id": "tSJWP0dRMSej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), strides=1, padding='same', activation='relu', input_shape=(150, 150, 1), name='conv2d_1'))\n",
        "model.add(BatchNormalization(name='batch_norm_1'))\n",
        "model.add(MaxPool2D((2, 2), strides=2, padding='same', name='maxpool_1'))\n",
        "model.add(Conv2D(64, (3, 3), strides=1, padding='same', activation='relu', name='conv2d_2'))\n",
        "model.add(Dropout(0.1, name='dropout_1'))\n",
        "model.add(BatchNormalization(name='batch_norm_2'))\n",
        "model.add(MaxPool2D((2, 2), strides=2, padding='same', name='maxpool_2'))\n",
        "model.add(Conv2D(64, (3, 3), strides=1, padding='same', activation='relu', name='conv2d_3'))\n",
        "model.add(BatchNormalization(name='batch_norm_3'))\n",
        "model.add(MaxPool2D((2, 2), strides=2, padding='same', name='maxpool_3'))\n",
        "model.add(Conv2D(128, (3, 3), strides=1, padding='same', activation='relu', name='conv2d_4'))\n",
        "model.add(Dropout(0.2, name='dropout_2'))\n",
        "model.add(BatchNormalization(name='batch_norm_4'))\n",
        "model.add(MaxPool2D((2, 2), strides=2, padding='same', name='maxpool_4'))\n",
        "model.add(Conv2D(256, (3, 3), strides=1, padding='same', activation='relu', name='conv2d_5'))\n",
        "model.add(Dropout(0.2, name='dropout_3'))\n",
        "model.add(BatchNormalization(name='batch_norm_5'))\n",
        "model.add(MaxPool2D((2, 2), strides=2, padding='same', name='maxpool_5'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=128, activation='relu', name='dense_1'))\n",
        "model.add(Dropout(0.2, name='dropout_4'))\n",
        "model.add(Dense(units=1, activation='sigmoid', name='dense_2'))\n",
        "model.compile(optimizer=\"rmsprop\", loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "pGxWrXUAMssZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "rTyfSXOhaqjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_selective_feature_dropping(model, images, labels, drop_ratio=0.2, keep_ratio=0.2):\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "        tape.watch(images)\n",
        "\n",
        "        outputs = model(images, training=True)\n",
        "\n",
        "        labels = tf.reshape(labels, (-1, 1))\n",
        "        loss = tf.keras.losses.binary_crossentropy(labels, outputs)\n",
        "\n",
        "    grads = tape.gradient(loss, images)\n",
        "\n",
        "    norm_grads = tf.norm(grads, axis=-1)\n",
        "\n",
        "    norm_grads_flat = tf.reshape(norm_grads, [tf.shape(norm_grads)[0], -1])\n",
        "\n",
        "    total_features = np.prod(images.shape[1:4])\n",
        "\n",
        "    keep_features = int(keep_ratio * total_features)\n",
        "    drop_features = int(drop_ratio * total_features)\n",
        "\n",
        "    sorted_values, sorted_indices = tf.math.top_k(norm_grads_flat, k=total_features)\n",
        "\n",
        "    drop_indices = sorted_indices[:, keep_features:keep_features+drop_features]\n",
        "\n",
        "    mask = tf.ones_like(norm_grads_flat)\n",
        "\n",
        "    batch_indices = tf.reshape(\n",
        "        tf.repeat(tf.range(tf.shape(norm_grads_flat)[0]), tf.shape(drop_indices)[1]), (-1, 1)\n",
        "    )\n",
        "    scatter_indices = tf.concat([batch_indices, tf.reshape(drop_indices, (-1, 1))], axis=1)\n",
        "\n",
        "    mask = tf.tensor_scatter_nd_update(mask, scatter_indices, tf.zeros(tf.shape(scatter_indices)[0]))\n",
        "\n",
        "    mask = tf.reshape(mask, tf.shape(images))\n",
        "\n",
        "    masked_images = tf.multiply(images, mask)\n",
        "\n",
        "    return masked_images"
      ],
      "metadata": {
        "id": "SvhqSzx64F5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=2, verbose=1, factor=0.3, min_lr=0.000001)\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "learning_rate_reduction.set_model(model)\n",
        "\n",
        "feature_ratio = 0.065\n",
        "\n",
        "def apply_feature_dropping(model, images, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "        tape.watch(images)\n",
        "\n",
        "        outputs = model(images, training=True)\n",
        "\n",
        "        labels = tf.reshape(labels, (-1, 1))\n",
        "        loss = tf.keras.losses.binary_crossentropy(labels, outputs)\n",
        "\n",
        "    grads = tape.gradient(loss, images)\n",
        "\n",
        "    norm_grads = tf.norm(grads, axis=-1)\n",
        "\n",
        "    norm_grads_flat = tf.reshape(norm_grads, [tf.shape(norm_grads)[0], -1])\n",
        "\n",
        "    neg_norm_grads_flat = -norm_grads_flat\n",
        "\n",
        "    _, idx_mask = tf.math.top_k(neg_norm_grads_flat, k=int(feature_ratio * np.prod(images.shape[1:4])), sorted=False)\n",
        "\n",
        "    mask = tf.ones_like(norm_grads_flat)\n",
        "\n",
        "    batch_indices = tf.reshape(\n",
        "        tf.repeat(tf.range(tf.shape(norm_grads_flat)[0]), tf.shape(idx_mask)[1]), (-1, 1)\n",
        "    )\n",
        "    scatter_indices = tf.concat([batch_indices, tf.reshape(idx_mask, (-1, 1))], axis=1)\n",
        "\n",
        "    mask = tf.tensor_scatter_nd_update(mask, scatter_indices, tf.zeros(tf.shape(scatter_indices)[0]))\n",
        "\n",
        "    mask = tf.reshape(mask, tf.shape(images))\n",
        "\n",
        "    masked_images = tf.multiply(images, mask)\n",
        "\n",
        "    return masked_images\n",
        "\n",
        "\n",
        "epochs = 12\n",
        "num_batches = len(x_train) // 32  # assuming batch_size of 32\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Starting Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "\n",
        "    pbar = tqdm(total=num_batches, ncols=70)\n",
        "\n",
        "    for i, (batch_x, batch_y) in enumerate(datagen.flow(x_train, y_train, batch_size=32)):\n",
        "        batch_x = tf.convert_to_tensor(batch_x, dtype=tf.float32)\n",
        "\n",
        "        batch_y = tf.convert_to_tensor(batch_y, dtype=tf.float32)\n",
        "\n",
        "        masked_images = apply_feature_dropping(model, batch_x, batch_y)\n",
        "\n",
        "        loss, acc = model.train_on_batch(masked_images, batch_y)\n",
        "\n",
        "        train_losses.append(loss)\n",
        "        train_accuracies.append(acc)\n",
        "\n",
        "        pbar.set_description(f'Epoch {epoch + 1}/{epochs}, Train Accuracy: {np.mean(train_accuracies):.4f}')\n",
        "        pbar.update()\n",
        "\n",
        "        if i + 1 == num_batches:\n",
        "            break\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    avg_train_loss = np.mean(train_losses)\n",
        "    avg_train_acc = np.mean(train_accuracies)\n",
        "\n",
        "    val_loss, val_acc = model.evaluate(datagen.flow(x_val, y_val), verbose=0)\n",
        "\n",
        "    learning_rate_reduction.on_epoch_end(epoch, {\"val_accuracy\": val_acc})\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Train Accuracy: {avg_train_acc:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "print(\"Training completed!\")\n"
      ],
      "metadata": {
        "id": "4qNEAAc4RxSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define the learning rate reduction callback\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=2, verbose=1, factor=0.3, min_lr=0.000001)\n"
      ],
      "metadata": {
        "id": "zcreBeyS42ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(x=train_masks,\n",
        "                     y=y_train,\n",
        "                     batch_size=32,\n",
        "                     epochs=20,\n",
        "                     validation_data=(x_val, y_val),\n",
        "                     callbacks=[learning_rate_reduction, early_stopping])\n",
        "\n",
        "print(\"Training completed!\")"
      ],
      "metadata": {
        "id": "zEAhPZ-YqtRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(datagen.flow(x_train, y_train, batch_size=32) ,epochs = 12 , validation_data = datagen.flow(x_val, y_val) ,callbacks = [learning_rate_reduction])"
      ],
      "metadata": {
        "id": "PSUgVD2VxmGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loss of the model is - \" , model.evaluate(x_test,y_test)[0])\n",
        "print(\"Accuracy of the model is - \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")"
      ],
      "metadata": {
        "id": "NX9uI6IisURc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('my_model.h5')"
      ],
      "metadata": {
        "id": "iXdo2yDOtLlE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}